{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_snippets.torch_loader import Report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Read_ply(Dataset):\n",
    "    def __init__(self, ply_path):\n",
    "        super(Read_ply, self).__init__()\n",
    "        self.ply_paths = ply_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ply_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        points = np.asarray(o3d.io.read_point_cloud(self.ply_paths[index]).points)\n",
    "        return points\n",
    "\n",
    "    def collate_fn(self, points):\n",
    "        max_points = max([p.shape[0] for p in points])  # Find the maximum number of points in the batch\n",
    "        \n",
    "        # Pad or truncate each point cloud to the same size (max_points)\n",
    "        padded_points = []\n",
    "        for point_cloud in points:\n",
    "            if point_cloud.shape[0] < max_points:\n",
    "                # Pad the point cloud with zeros to match the maximum size\n",
    "                padded = np.pad(point_cloud, ((0, max_points - point_cloud.shape[0]), (0, 0)), mode='constant')\n",
    "                padded_points.append(padded)\n",
    "\n",
    "        # Stack the point clouds to form a batch\n",
    "        points = np.stack(padded_points, axis=0)  # (B, N, 3)\n",
    "\n",
    "         # Create the target as the next frame\n",
    "        target = np.roll(points, shift=-1, axis=0)  # Shift the points by one frame along the batch dimension\n",
    "        target[-1] = points[-1]  # Set the last target to the last frame (no next frame for it)\n",
    "\n",
    "        # Convert to tensors\n",
    "        points = torch.tensor(points).float()\n",
    "        target = torch.tensor(target).float()\n",
    "\n",
    "        return points, target              \n",
    "\n",
    "def RandomSplit(datasets, train_set_percentage):\n",
    "    lengths = [int(len(datasets)*train_set_percentage), len(datasets)-int(len(datasets)*train_set_percentage)]\n",
    "    return random_split(datasets, lengths)\n",
    "\n",
    "\n",
    "def GetDataLoader(ply_paths, batch_size, train_set_percentage=0.9, shuffle=True, drop_last=True):\n",
    "    # Defining the dataset\n",
    "    ds = Read_ply(ply_paths)\n",
    "    # Randomly splitting the dataset\n",
    "    train_set, val_set = RandomSplit(ds, train_set_percentage)\n",
    "\n",
    "    # Defining the dataloader\n",
    "    val_dl = DataLoader(val_set, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, collate_fn=ds.collate_fn)\n",
    "    train_dl = DataLoader(train_set, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, collate_fn=ds.collate_fn)\n",
    "    \n",
    "    return train_dl, val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2390121, 3]) torch.Size([1, 2390121, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ply_paths = [f'ped_4ff8af4d-6840-47c2-bc9b-eb383009ad65/frame_{i}.ply' for i in range(0,20,2)]\n",
    "\n",
    "train_dl, val_dl = GetDataLoader(ply_paths, 2)\n",
    "\n",
    "for batch in train_dl:\n",
    "    print(batch[0].shape, batch[1].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, embed_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(3, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, embed_dim)  # Transform to higher-dimensional space\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input shape x: (batch_size, num_points, 3)\n",
    "        Output shape: (batch_size, num_points, embed_dim)\n",
    "        \"\"\"\n",
    "        out = self.mlp(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttentionMLP(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_size):\n",
    "        super(AttentionMLP, self).__init__()\n",
    "        self.att_mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, 1), # Scalar score  for each point\n",
    "            nn.Softmax(dim=1) # Normalize scores across points\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input shape x: (batch_size, num_points, embed_dim)\n",
    "        Output shape: (batch_size, num_points, 1)\n",
    "        \"\"\"\n",
    "        out = self.att_mlp(x)\n",
    "        return out\n",
    "    \n",
    "class AttentionVector(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_size, num_heads=2, dropout=0.1):\n",
    "        super(AttentionVector, self).__init__()\n",
    "        self.feature_mlp = MLP(hidden_size, embed_dim)\n",
    "        self.att_mlp = AttentionMLP(embed_dim, hidden_size)\n",
    "        # self.mha = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input shape x: (batch_size, num_points, 3)\n",
    "        Output shape: (batch_size, embed_dim)\n",
    "        \"\"\"\n",
    "        features = self.feature_mlp(x) # -> (batch_size, num_points, embed_dim)\n",
    "        attention_scores = self.att_mlp(features) # -> (batch_size, num_points, 1)\n",
    "        wei = attention_scores * features # -> (batch_size, num_points, embed_dim)\n",
    "\n",
    "        return wei\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233 parameters\n"
     ]
    }
   ],
   "source": [
    "model = AttentionVector(embed_dim=16, hidden_size=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()), \"parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, optimizer, criterion, tr_dl):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for input, target in tr_dl:    \n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad(set_to_none= True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        average_loss = total_loss / len(tr_dl)\n",
    "\n",
    "    return average_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_batch(model, criterion, val_dl):\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    for input, target in val_dl:\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss.item()\n",
    "        average_loss = total_loss / len(val_dl)\n",
    "\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "log = Report(n_epochs)\n",
    "for epoch in range(n_epochs):\n",
    "  N=len(train_dl)\n",
    "\n",
    "  for ix, _ in enumerate(train_dl):\n",
    "    avg_loss = train_batch(model, optimizer, criterion, train_dl)\n",
    "    log.record(epoch+(ix+1)/N, trn_loss= avg_loss, end='\\r')\n",
    "\n",
    "  val_loss=0\n",
    "  N=len(val_dl)\n",
    "  for ix, _ in enumerate(val_dl):\n",
    "\n",
    "    loss= val_batch(val_dl, model)\n",
    "    val_loss+= loss\n",
    "    log.record(epoch+(ix+1)/N, val_loss=loss, end='\\r')\n",
    "\n",
    "  log.report_avgs(epoch+1)\n",
    "log.plot_epochs(['trn_loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2796887, 3])\n",
      "torch.Size([1, 2796887, 16])\n"
     ]
    }
   ],
   "source": [
    "pcd_points = o3d.io.read_point_cloud(r\"ped_4ff8af4d-6840-47c2-bc9b-eb383009ad65\\frame_0.ply\").points\n",
    "\n",
    "pcd_points = torch.tensor(np.asarray(pcd_points), dtype=torch.float32)\n",
    "batch_size = 1\n",
    "pcd_points = pcd_points.view(batch_size, -1, 3)\n",
    "print(pcd_points.shape)\n",
    "model = AttentionVector(embed_dim=16, hidden_size=32)\n",
    "\n",
    "out = model(pcd_points)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-6.3823e-07,  5.1540e-07,  3.0813e-07, -4.2532e-07, -1.2801e-07,\n",
      "         4.9915e-07, -8.1783e-07, -6.9563e-07, -8.2315e-07,  2.2232e-07,\n",
      "         8.4126e-07,  3.1474e-07,  3.0252e-07,  1.0896e-06,  9.1309e-07,\n",
      "        -5.1847e-07], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2796887, 3) float64\n"
     ]
    }
   ],
   "source": [
    "np_points = (out).detach().numpy()[0]\n",
    "np_points = np_points[:,:3]\n",
    "np_points=np_points.astype(np.float64)\n",
    "print(np_points.shape, np_points.dtype)\n",
    "\n",
    "points = o3d.utility.Vector3dVector(np_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: The requested transformation operation is not supported. \n"
     ]
    }
   ],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = points\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
