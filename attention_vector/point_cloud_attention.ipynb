{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader, random_split\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_snippets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Report\n",
      "File \u001b[1;32mc:\\Users\\youss\\miniconda3\\envs\\pcp-env\\Lib\\site-packages\\torch_snippets\\torch_loader.py:40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mth\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mT\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mloguru\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n",
      "File \u001b[1;32mc:\\Users\\youss\\miniconda3\\envs\\pcp-env\\Lib\\site-packages\\torchvision\\__init__.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\youss\\miniconda3\\envs\\pcp-env\\Lib\\site-packages\\torchvision\\_meta_registrations.py:25\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroi_align\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\youss\\miniconda3\\envs\\pcp-env\\Lib\\site-packages\\torchvision\\_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[0;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_snippets.torch_loader import Report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Read_ply(Dataset):\n",
    "    def __init__(self, ply_path):\n",
    "        super(Read_ply, self).__init__()\n",
    "        self.ply_paths = ply_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ply_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        points = np.asarray(o3d.io.read_point_cloud(self.ply_paths[index]).points)\n",
    "        return points\n",
    "\n",
    "    def collate_fn(self, points):\n",
    "        max_points = max([p.shape[0] for p in points])  # Find the maximum number of points in the batch\n",
    "        \n",
    "        # Pad or truncate each point cloud to the same size (max_points)\n",
    "        padded_points = []\n",
    "        for point_cloud in points:\n",
    "            if point_cloud.shape[0] < max_points:\n",
    "                # Pad the point cloud with zeros to match the maximum size\n",
    "                padded = np.pad(point_cloud, ((0, max_points - point_cloud.shape[0]), (0, 0)), mode='constant')\n",
    "                padded_points.append(padded)\n",
    "\n",
    "        # Stack the point clouds to form a batch\n",
    "        points = np.stack(padded_points, axis=0)  # (B, N, 3)\n",
    "\n",
    "         # Create the target as the next frame\n",
    "        target = np.roll(points, shift=-1, axis=0)  # Shift the points by one frame along the batch dimension\n",
    "        target[-1] = points[-1]  # Set the last target to the last frame (no next frame for it)\n",
    "\n",
    "        # Convert to tensors\n",
    "        points = torch.tensor(points).float()\n",
    "        target = torch.tensor(target).float()\n",
    "\n",
    "        return points, target              \n",
    "\n",
    "def RandomSplit(datasets, train_set_percentage):\n",
    "    lengths = [int(len(datasets)*train_set_percentage), len(datasets)-int(len(datasets)*train_set_percentage)]\n",
    "    return random_split(datasets, lengths)\n",
    "\n",
    "\n",
    "def GetDataLoader(ply_paths, batch_size, train_set_percentage=0.9, shuffle=True, drop_last=True):\n",
    "    # Defining the dataset\n",
    "    ds = Read_ply(ply_paths)\n",
    "    # Randomly splitting the dataset\n",
    "    train_set, val_set = RandomSplit(ds, train_set_percentage)\n",
    "\n",
    "    # Defining the dataloader\n",
    "    val_dl = DataLoader(val_set, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, collate_fn=ds.collate_fn)\n",
    "    train_dl = DataLoader(train_set, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, collate_fn=ds.collate_fn)\n",
    "    \n",
    "    return train_dl, val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2390121, 3]) torch.Size([1, 2390121, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ply_paths = [f'ped_4ff8af4d-6840-47c2-bc9b-eb383009ad65/frame_{i}.ply' for i in range(0,20,2)]\n",
    "\n",
    "train_dl, val_dl = GetDataLoader(ply_paths, 2)\n",
    "\n",
    "for batch in train_dl:\n",
    "    print(batch[0].shape, batch[1].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, embed_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(3, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, embed_dim)  # Transform to higher-dimensional space\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input shape x: (batch_size, num_points, 3)\n",
    "        Output shape: (batch_size, num_points, embed_dim)\n",
    "        \"\"\"\n",
    "        out = self.mlp(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttentionMLP(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_size):\n",
    "        super(AttentionMLP, self).__init__()\n",
    "        self.att_mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, 1), # Scalar score  for each point\n",
    "            nn.Softmax(dim=1) # Normalize scores across points\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input shape x: (batch_size, num_points, embed_dim)\n",
    "        Output shape: (batch_size, num_points, 1)\n",
    "        \"\"\"\n",
    "        out = self.att_mlp(x)\n",
    "        return out\n",
    "    \n",
    "class AttentionVector(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_size, num_heads=2, dropout=0.1):\n",
    "        super(AttentionVector, self).__init__()\n",
    "        self.feature_mlp = MLP(hidden_size, embed_dim)\n",
    "        self.att_mlp = AttentionMLP(embed_dim, hidden_size)\n",
    "        # self.mha = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input shape x: (batch_size, num_points, 3)\n",
    "        Output shape: (batch_size, embed_dim)\n",
    "        \"\"\"\n",
    "        features = self.feature_mlp(x) # -> (batch_size, num_points, embed_dim)\n",
    "        attention_scores = self.att_mlp(features) # -> (batch_size, num_points, 1)\n",
    "        wei = attention_scores * features # -> (batch_size, num_points, embed_dim)\n",
    "\n",
    "        return wei\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233 parameters\n"
     ]
    }
   ],
   "source": [
    "model = AttentionVector(embed_dim=16, hidden_size=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()), \"parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, optimizer, criterion, tr_dl):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for input, target in tr_dl:    \n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad(set_to_none= True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        average_loss = total_loss / len(tr_dl)\n",
    "\n",
    "    return average_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_batch(model, criterion, val_dl):\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    for input, target in val_dl:\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss.item()\n",
    "        average_loss = total_loss / len(val_dl)\n",
    "\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m----> 2\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[43mReport\u001b[49m(n_epochs)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m      4\u001b[0m   N\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dl)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Report' is not defined"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "log = Report(n_epochs)\n",
    "for epoch in range(n_epochs):\n",
    "  N=len(train_dl)\n",
    "\n",
    "  for ix, _ in enumerate(train_dl):\n",
    "    avg_loss = train_batch(model, optimizer, criterion, train_dl)\n",
    "    log.record(epoch+(ix+1)/N, trn_loss= avg_loss, end='\\r')\n",
    "\n",
    "  val_loss=0\n",
    "  N=len(val_dl)\n",
    "  for ix, _ in enumerate(val_dl):\n",
    "\n",
    "    loss= val_batch(val_dl, model)\n",
    "    val_loss+= loss\n",
    "    log.record(epoch+(ix+1)/N, val_loss=loss, end='\\r')\n",
    "\n",
    "  log.report_avgs(epoch+1)\n",
    "log.plot_epochs(['trn_loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2796887, 3])\n",
      "torch.Size([1, 2796887, 16])\n"
     ]
    }
   ],
   "source": [
    "pcd_points = o3d.io.read_point_cloud(r\"ped_4ff8af4d-6840-47c2-bc9b-eb383009ad65\\frame_0.ply\").points\n",
    "\n",
    "pcd_points = torch.tensor(np.asarray(pcd_points), dtype=torch.float32)\n",
    "batch_size = 1\n",
    "pcd_points = pcd_points.view(batch_size, -1, 3)\n",
    "print(pcd_points.shape)\n",
    "model = AttentionVector(embed_dim=16, hidden_size=32)\n",
    "\n",
    "out = model(pcd_points)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-6.3823e-07,  5.1540e-07,  3.0813e-07, -4.2532e-07, -1.2801e-07,\n",
      "         4.9915e-07, -8.1783e-07, -6.9563e-07, -8.2315e-07,  2.2232e-07,\n",
      "         8.4126e-07,  3.1474e-07,  3.0252e-07,  1.0896e-06,  9.1309e-07,\n",
      "        -5.1847e-07], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2796887, 3) float64\n"
     ]
    }
   ],
   "source": [
    "np_points = (out).detach().numpy()[0]\n",
    "np_points = np_points[:,:3]\n",
    "np_points=np_points.astype(np.float64)\n",
    "print(np_points.shape, np_points.dtype)\n",
    "\n",
    "points = o3d.utility.Vector3dVector(np_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: The requested transformation operation is not supported. \n"
     ]
    }
   ],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = points\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
