{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Save Speed and Distance Features in Json file"
      ],
      "metadata": {
        "id": "_MXqJpbv3is2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import json"
      ],
      "metadata": {
        "id": "rDQQZdewxgZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_odometry(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        values = list(map(float, f.readline().strip().split(\",\")))\n",
        "        return {\"x\": values[0], \"y\": values[1], \"z\": values[2]}\n",
        "\n",
        "def parse_labels(file_path):\n",
        "    pedestrians = {}\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\",\")\n",
        "            if parts[0] == \"Pedestrian\":\n",
        "                ped_id = parts[1]\n",
        "                pos_x = float(parts[3])\n",
        "                pos_y = float(parts[4])\n",
        "                pedestrians[ped_id] = {\"x\": pos_x, \"y\": pos_y}\n",
        "    return pedestrians"
      ],
      "metadata": {
        "id": "F2bcJf_TxrNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_speed_distance_movement(prev_position, curr_position, ego_delta):\n",
        "    \"\"\"\n",
        "    Calculate speed, distance, and movement status for a pedestrian relative to the ego vehicle.\n",
        "    \"\"\"\n",
        "    ped_x, ped_y = curr_position\n",
        "    prev_x, prev_y = prev_position\n",
        "\n",
        "    # Distance calculation\n",
        "    distance = math.sqrt(ped_x**2 + ped_y**2)\n",
        "\n",
        "    # Speed calculation\n",
        "    movement = math.sqrt((ped_x - prev_x)**2 + (ped_y - prev_y)**2)\n",
        "    adjusted_speed = max(0, movement - ego_delta)  # Adjust for ego movement\n",
        "\n",
        "    # Movement status\n",
        "    movement_status = \"Stopped\" if adjusted_speed < 0.25 else \"Moving\"\n",
        "\n",
        "    return adjusted_speed, distance, movement_status"
      ],
      "metadata": {
        "id": "xESptC2Fx5Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MUFcv5Fxam3"
      },
      "outputs": [],
      "source": [
        "def process_scenario(scenario_path, scenario_name):\n",
        "    \"\"\"\n",
        "    Process a single scenario to extract features for each pedestrian,\n",
        "    including corrections for initial \"Unknown\" movement status.\n",
        "    \"\"\"\n",
        "    frame_files = sorted(os.listdir(scenario_path))\n",
        "    odom_files = [f for f in frame_files if f.startswith(\"odom\")]\n",
        "    label_files = [f for f in frame_files if f.startswith(\"label3d\")]\n",
        "\n",
        "    # Track data\n",
        "    previous_positions = {}\n",
        "    pending_corrections = {}\n",
        "    perv_ego_position = None\n",
        "\n",
        "    scenario_features = {}\n",
        "\n",
        "    for odom_file, label_file in zip(odom_files, label_files):\n",
        "        cleaned_name = odom_file.split(\"_\")[1].split(\".\")[0].split()[0]\n",
        "        frame_id = int(cleaned_name)\n",
        "\n",
        "        odom_path = os.path.join(scenario_path, odom_file)\n",
        "        label_path = os.path.join(scenario_path, label_file)\n",
        "\n",
        "        # Parse current odometry and labels\n",
        "        ego_position = parse_odometry(odom_path)\n",
        "        pedestrians = parse_labels(label_path)\n",
        "\n",
        "        # Calculate ego movement delta (if applicable)\n",
        "        ego_delta = 0\n",
        "        if perv_ego_position:\n",
        "            ego_delta = math.sqrt(\n",
        "                (ego_position[\"x\"] - perv_ego_position[\"x\"])**2 +\n",
        "                (ego_position[\"y\"] - perv_ego_position[\"y\"])**2\n",
        "            )\n",
        "\n",
        "        # Collect features for this frame\n",
        "        frame_features = {}\n",
        "\n",
        "        for ped_id, ped_data in pedestrians.items():\n",
        "            ped_x, ped_y = ped_data[\"x\"], ped_data[\"y\"]\n",
        "\n",
        "            # Ignore pedestrians behind the ego vehicle\n",
        "            if ped_x < 0:\n",
        "                continue\n",
        "\n",
        "            if ped_id in previous_positions:\n",
        "                # Calculate speed, distance, and movement status\n",
        "                prev_position = previous_positions[ped_id]\n",
        "                speed, distance, movement_status = calculate_speed_distance_movement(\n",
        "                    prev_position, (ped_x, ped_y), ego_delta\n",
        "                )\n",
        "\n",
        "                # Remove pending corrections for this pedestrian if applicable\n",
        "                if ped_id in pending_corrections:\n",
        "                    pending_frame_id = pending_corrections[ped_id]\n",
        "                    del scenario_features[pending_frame_id][ped_id]\n",
        "                    del pending_corrections[ped_id]\n",
        "\n",
        "            else:\n",
        "                # Initialize for the first frame\n",
        "                speed, distance = 0, math.sqrt(ped_x**2 + ped_y**2)\n",
        "                movement_status = \"Unknown\"\n",
        "\n",
        "                # Add this pedestrian to pending corrections\n",
        "                pending_corrections[ped_id] = frame_id\n",
        "\n",
        "            # Save current position for next frame\n",
        "            previous_positions[ped_id] = (ped_x, ped_y)\n",
        "\n",
        "            # Store features for this pedestrian\n",
        "            frame_features[ped_id] = {\n",
        "                \"speed\": speed,\n",
        "                \"distance\": distance,\n",
        "                \"movement_status\": movement_status\n",
        "            }\n",
        "\n",
        "        # Update scenario features\n",
        "        scenario_features[frame_id] = frame_features\n",
        "\n",
        "        # Update previous ego position\n",
        "        perv_ego_position = ego_position\n",
        "\n",
        "    return scenario_features\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_all_scenarios(root_directory):\n",
        "    \"\"\"\n",
        "    Process all scenarios in the dataset and save results in JSON files in the `output_features` folder.\n",
        "    \"\"\"\n",
        "    # Ensure the output directory exists\n",
        "    output_directory = os.path.join(root_directory, \"output_features\")\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "    for scenario in os.listdir(root_directory):\n",
        "        scenario_path = os.path.join(root_directory, scenario)\n",
        "        if not os.path.isdir(scenario_path):\n",
        "            continue\n",
        "\n",
        "        # Process the scenario\n",
        "        scenario_features = process_scenario(scenario_path, scenario)\n",
        "\n",
        "        # Save scenario features to a JSON file\n",
        "        output_path = os.path.join(output_directory, f\"{scenario}_features.json\")\n",
        "        with open(output_path, \"w\") as json_file:\n",
        "            json.dump(scenario_features, json_file, indent=4)\n",
        "\n",
        "        print(f\"Saved features for {scenario} to {output_path}\")"
      ],
      "metadata": {
        "id": "BBiKzq7wyAFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run the processing function\n",
        "root_directory = \"/content/drive/MyDrive/Loki_Dataset/Loki\"\n",
        "process_all_scenarios(root_directory)\n",
        "\n",
        "print(\"Feature extraction complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32VN_LhkyAHT",
        "outputId": "74d21b71-a8a0-4399-ced7-5bb9961460ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features for scenario_000 to /content/drive/MyDrive/Loki_Dataset/Loki/output_features/scenario_000_features.json\n",
            "Saved features for scenario_026 to /content/drive/MyDrive/Loki_Dataset/Loki/output_features/scenario_026_features.json\n",
            "Saved features for scenario_014 to /content/drive/MyDrive/Loki_Dataset/Loki/output_features/scenario_014_features.json\n",
            "Saved features for output_features to /content/drive/MyDrive/Loki_Dataset/Loki/output_features/output_features_features.json\n",
            "Feature extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "27iHip7ozSLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine Features"
      ],
      "metadata": {
        "id": "MBjRmLC63qwf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GQyX9Lew3hMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_features(group_walking_file, speed_distance_file, output_file):\n",
        "    \"\"\"\n",
        "    Merge two JSON files for a scenario based on pedestrian IDs.\n",
        "    \"\"\"\n",
        "    # Load JSON files\n",
        "    with open(group_walking_file, \"r\") as gw_file:\n",
        "        group_walking_data = json.load(gw_file)\n",
        "\n",
        "    with open(speed_distance_file, \"r\") as sd_file:\n",
        "        speed_distance_data = json.load(sd_file)\n",
        "\n",
        "    # Initialize merged data\n",
        "    merged_data = {}\n",
        "\n",
        "    # Iterate through frames in both files\n",
        "    for frame_id, ped_data_gw in group_walking_data.items():\n",
        "        if frame_id in speed_distance_data:\n",
        "            ped_data_sd = speed_distance_data[frame_id]\n",
        "            merged_frame = {}\n",
        "\n",
        "            # Match pedestrian IDs\n",
        "            for ped_id, features_gw in ped_data_gw.items():\n",
        "                if ped_id in ped_data_sd:\n",
        "                    features_sd = ped_data_sd[ped_id]\n",
        "\n",
        "                    # Concatenate features\n",
        "                    merged_frame[ped_id] = {\n",
        "                        \"group_status\": features_gw[\"group_status\"],\n",
        "                        \"walking_toward_vehicle\": features_gw[\"walking_toward_vehicle\"],\n",
        "                        \"speed\": features_sd[\"speed\"],\n",
        "                        \"distance\": features_sd[\"distance\"],\n",
        "                        \"movement_status\": features_sd[\"movement_status\"]\n",
        "                    }\n",
        "\n",
        "            # Add merged frame data\n",
        "            if merged_frame:\n",
        "                merged_data[frame_id] = merged_frame\n",
        "\n",
        "    # Save merged data to a new JSON file\n",
        "    with open(output_file, \"w\") as out_file:\n",
        "        json.dump(merged_data, out_file, indent=4)\n",
        "\n",
        "    print(f\"Merged features saved to {output_file}\")"
      ],
      "metadata": {
        "id": "E9owuHUT3hPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_all_scenarios(group_walking_dir, speed_distance_dir, output_dir):\n",
        "    \"\"\"\n",
        "    Process all scenarios by merging group/walking and speed/distance JSON files.\n",
        "    \"\"\"\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Process each scenario\n",
        "    for filename in os.listdir(group_walking_dir):\n",
        "        if filename.endswith(\"_features.json\"):\n",
        "            # Derive corresponding file paths\n",
        "            scenario_name = filename.replace(\"_features.json\", \"\")\n",
        "            group_walking_file = os.path.join(group_walking_dir, filename)\n",
        "            speed_distance_file = os.path.join(speed_distance_dir, f\"{scenario_name}_features.json\")\n",
        "            output_file = os.path.join(output_dir, f\"{scenario_name}_merged_features.json\")\n",
        "\n",
        "            # Only merge if both files exist\n",
        "            if os.path.exists(speed_distance_file):\n",
        "                merge_features(group_walking_file, speed_distance_file, output_file)\n",
        "            else:\n",
        "                print(f\"Missing speed/distance file for scenario: {scenario_name}\")"
      ],
      "metadata": {
        "id": "fkoo60zD4U6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define directories\n",
        "group_walking_dir = \"/content/drive/MyDrive/Loki_Dataset/output_features_Group & Walking\"\n",
        "speed_distance_dir = \"/content/drive/MyDrive/Loki_Dataset/output_features_Speed & Distance\"\n",
        "output_dir = \"/content/drive/MyDrive/Loki_Dataset/output_merged_jsons\"\n",
        "\n",
        "# Run the merging process\n",
        "process_all_scenarios(group_walking_dir, speed_distance_dir, output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S6AfKio4YYl",
        "outputId": "c19a2094-64df-496f-c77c-2e8f89657578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged features saved to /content/drive/MyDrive/Loki_Dataset/output_merged_jsons/scenario_000_merged_features.json\n",
            "Merged features saved to /content/drive/MyDrive/Loki_Dataset/output_merged_jsons/scenario_026_merged_features.json\n",
            "Merged features saved to /content/drive/MyDrive/Loki_Dataset/output_merged_jsons/scenario_014_merged_features.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mj5Y2Vh45oWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c6ZkZGmBkt8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention"
      ],
      "metadata": {
        "id": "A1x24LDmkvNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "kAaGlBGTvl_Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Multi-Head Attention Class\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert input_dim % num_heads == 0, \"Input dimension must be divisible by the number of heads\"\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = input_dim // num_heads\n",
        "        self.query = nn.Linear(input_dim, input_dim)\n",
        "        self.key = nn.Linear(input_dim, input_dim)\n",
        "        self.value = nn.Linear(input_dim, input_dim)\n",
        "        self.out = nn.Linear(input_dim, input_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_pedestrians, input_dim = x.shape\n",
        "        Q = self.query(x).view(batch_size, num_pedestrians, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.key(x).view(batch_size, num_pedestrians, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.value(x).view(batch_size, num_pedestrians, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        weighted_values = torch.matmul(self.dropout(attention_weights), V)\n",
        "\n",
        "        weighted_values = weighted_values.transpose(1, 2).contiguous().view(batch_size, num_pedestrians, input_dim)\n",
        "        output = self.out(weighted_values)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "metadata": {
        "id": "XNfnFZsssT1p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Attention Results\n",
        "def save_attention_results(output, attention_weights, output_file):\n",
        "    \"\"\"\n",
        "    Save weighted output and attention weights to a JSON file.\n",
        "    \"\"\"\n",
        "    output_data = {\n",
        "        \"weighted_output\": output.detach().cpu().numpy().tolist(),\n",
        "        \"attention_weights\": attention_weights.detach().cpu().numpy().tolist()\n",
        "    }\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(output_data, f, indent=4)"
      ],
      "metadata": {
        "id": "Qh91hLyJvfCv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Preprocess Features from JSON\n",
        "def load_features_from_json(json_file):\n",
        "    \"\"\"\n",
        "    Load and preprocess features from a JSON file.\n",
        "    \"\"\"\n",
        "    with open(json_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    features = []\n",
        "\n",
        "    # Collect speed and distance values for scaling\n",
        "    all_speeds = []\n",
        "    all_distances = []\n",
        "    for frame_id, pedestrians in data.items():\n",
        "        for ped_id, ped_features in pedestrians.items():\n",
        "            all_speeds.append(ped_features.get(\"speed\", 0.0))\n",
        "            all_distances.append(ped_features.get(\"distance\", 0.0))\n",
        "\n",
        "    # Fit scalers\n",
        "    speed_scaler = MinMaxScaler()\n",
        "    distance_scaler = MinMaxScaler()\n",
        "    speed_scaler.fit(np.array(all_speeds).reshape(-1, 1))\n",
        "    distance_scaler.fit(np.array(all_distances).reshape(-1, 1))\n",
        "\n",
        "    # Process features\n",
        "    for frame_id, pedestrians in data.items():\n",
        "        for ped_id, ped_features in pedestrians.items():\n",
        "            group_status = ped_features.get(\"group_status\", 0)\n",
        "            walking_toward_vehicle = ped_features.get(\"walking_toward_vehicle\", 0)\n",
        "            speed = speed_scaler.transform([[ped_features.get(\"speed\", 0.0)]])[0][0]\n",
        "            distance = distance_scaler.transform([[ped_features.get(\"distance\", 0.0)]])[0][0]\n",
        "            movement_status = 1 if ped_features.get(\"movement_status\", \"Stopped\") == \"Moving\" else 0\n",
        "\n",
        "            # Combine features\n",
        "            features.append([group_status, walking_toward_vehicle, speed, distance, movement_status])\n",
        "\n",
        "    # Convert to tensor and add batch dimension\n",
        "    features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0)\n",
        "    return features_tensor"
      ],
      "metadata": {
        "id": "t5wRH1sivqpj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Apply Attention to Each Scenario\n",
        "def process_scenarios(input_folder, output_folder, input_dim, num_heads):\n",
        "    \"\"\"\n",
        "    Apply Multi-Head Attention on preprocessed features for each scenario JSON file and save results.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    attention_layer = MultiHeadAttention(input_dim=input_dim, num_heads=num_heads)\n",
        "\n",
        "    for json_file in os.listdir(input_folder):\n",
        "        if json_file.endswith('.json'):\n",
        "            input_path = os.path.join(input_folder, json_file)\n",
        "            output_path = os.path.join(output_folder, f\"attention_{json_file}\")\n",
        "\n",
        "            # Preprocess features\n",
        "            features = load_features_from_json(input_path)\n",
        "            print(f\"Processing {json_file} with features shape: {features.shape}\")\n",
        "\n",
        "            # Apply attention\n",
        "            attention_output, attention_weights = attention_layer(features)\n",
        "\n",
        "            # Save results\n",
        "            save_attention_results(attention_output, attention_weights, output_path)\n",
        "            print(f\"Saved attention results for {json_file} to {output_path}\")\n"
      ],
      "metadata": {
        "id": "SHahNLB1vq0n"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "input_folder = \"/content/drive/MyDrive/Loki_Dataset/output_merged_jsons\"  # Folder containing JSON files for each scenario\n",
        "output_folder = \"/content/drive/MyDrive/Loki_Dataset/attention_results\"  # Folder to save attention results\n",
        "input_dim = 5  # Number of input features (group_status, walking_toward_vehicle, scaled_speed, scaled_distance, movement_status)\n",
        "num_heads = 5  # Number of attention heads\n",
        "\n",
        "process_scenarios(input_folder, output_folder, input_dim, num_heads)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JikA_ZwfvfJQ",
        "outputId": "0282dd6e-1326-423b-a860-6b9e1613a206"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing scenario_000_merged_features.json with features shape: torch.Size([1, 260, 5])\n",
            "Saved attention results for scenario_000_merged_features.json to /content/drive/MyDrive/Loki_Dataset/attention_results/attention_scenario_000_merged_features.json\n",
            "Processing scenario_026_merged_features.json with features shape: torch.Size([1, 293, 5])\n",
            "Saved attention results for scenario_026_merged_features.json to /content/drive/MyDrive/Loki_Dataset/attention_results/attention_scenario_026_merged_features.json\n",
            "Processing scenario_014_merged_features.json with features shape: torch.Size([1, 1626, 5])\n",
            "Saved attention results for scenario_014_merged_features.json to /content/drive/MyDrive/Loki_Dataset/attention_results/attention_scenario_014_merged_features.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bpKXd1vxySf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Features from Point Cloud"
      ],
      "metadata": {
        "id": "MGfpb9KgU4xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install open3d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PArMBYj2XZ0V",
        "outputId": "1e678c56-b01a-497f-ad14-8823028c5eeb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting open3d\n",
            "  Downloading open3d-0.18.0-cp310-cp310-manylinux_2_27_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.26.4)\n",
            "Collecting dash>=2.6.0 (from open3d)\n",
            "  Downloading dash-2.18.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: werkzeug>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.1.3)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (5.10.4)\n",
            "Collecting configargparse (from open3d)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ipywidgets>=8.0.4 (from open3d)\n",
            "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting addict (from open3d)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (11.0.0)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.8.0)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (2.2.2)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from open3d) (6.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open3d) (4.67.1)\n",
            "Collecting pyquaternion (from open3d)\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting Flask<3.1,>=1.0.4 (from dash>=2.6.0->open3d)\n",
            "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting werkzeug>=2.2.3 (from open3d)\n",
            "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\n",
            "Collecting dash-html-components==2.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting dash-table==5.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (4.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.32.3)\n",
            "Collecting retrying (from dash>=2.6.0->open3d)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (75.1.0)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.12 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (2.8.2)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (5.7.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=2.2.3->open3d) (3.0.2)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (1.9.0)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.22.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.6)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2024.12.14)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n",
            "Downloading open3d-0.18.0-cp310-cp310-manylinux_2_27_x86_64.whl (399.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash-2.18.2-py3-none-any.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dash-table, dash-html-components, dash-core-components, addict, widgetsnbextension, werkzeug, retrying, pyquaternion, jedi, configargparse, comm, Flask, ipywidgets, dash, open3d\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.10\n",
            "    Uninstalling widgetsnbextension-3.6.10:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.10\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "  Attempting uninstall: Flask\n",
            "    Found existing installation: Flask 3.1.0\n",
            "    Uninstalling Flask-3.1.0:\n",
            "      Successfully uninstalled Flask-3.1.0\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed Flask-3.0.3 addict-2.4.0 comm-0.2.2 configargparse-1.7 dash-2.18.2 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 ipywidgets-8.1.5 jedi-0.19.2 open3d-0.18.0 pyquaternion-0.9.9 retrying-1.3.4 werkzeug-3.0.6 widgetsnbextension-4.0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import uuid\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import open3d as o3d\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "s4vTTYDVW1hI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset for Batch Processing\n",
        "class PedestrianPointCloudDataset(Dataset):\n",
        "    def __init__(self, ply_folder):\n",
        "        \"\"\"\n",
        "        Initialize dataset with pedestrian point cloud files.\n",
        "        Args:\n",
        "            ply_folder (str): Path to the folder containing .ply files for pedestrians.\n",
        "        \"\"\"\n",
        "        self.ply_files = [os.path.join(ply_folder, f) for f in os.listdir(ply_folder) if f.endswith(\".ply\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ply_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Load and normalize a pedestrian point cloud.\n",
        "        Args:\n",
        "            idx (int): Index of the pedestrian .ply file.\n",
        "        Returns:\n",
        "            tuple: (file_name, normalized_points)\n",
        "        \"\"\"\n",
        "        file_name = self.ply_files[idx]\n",
        "        points = load_and_normalize_ply(file_name)\n",
        "        return file_name, points"
      ],
      "metadata": {
        "id": "pwPaIc3GVTw0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Normalize Points from .ply\n",
        "def load_and_normalize_ply(ply_file):\n",
        "    \"\"\"\n",
        "    Load and normalize 3D points from a .ply file.\n",
        "    Args:\n",
        "        ply_file (str): Path to the .ply file.\n",
        "    Returns:\n",
        "        np.ndarray: Normalized 3D points of shape (N, 3).\n",
        "    \"\"\"\n",
        "    # Load the .ply file using Open3D\n",
        "    point_cloud = o3d.io.read_point_cloud(ply_file)\n",
        "    points = np.asarray(point_cloud.points)  # Shape: (N, 3)\n",
        "\n",
        "    # Normalize points to [0, 1] using Min-Max scaling\n",
        "    scaler = MinMaxScaler()\n",
        "    normalized_points = scaler.fit_transform(points)\n",
        "\n",
        "    return normalized_points"
      ],
      "metadata": {
        "id": "5OzZKMGxWyvD"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated PointNet Feature Extractor\n",
        "class PointNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, input_dim=3, output_dim=64):\n",
        "        super(PointNetFeatureExtractor, self).__init__()\n",
        "        self.mlp1 = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256)\n",
        "        )\n",
        "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, points):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            points (torch.Tensor): Tensor of shape [B, N, 3] (Batch, Num Points, Features)\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor of shape [B, output_dim] (Aggregated Features for each pedestrian)\n",
        "        \"\"\"\n",
        "        B, N, _ = points.shape\n",
        "        x = self.mlp1(points)  # Shape: [B, N, 256]\n",
        "        x = x.transpose(1, 2)  # Shape: [B, 256, N]\n",
        "        x = self.global_pool(x).squeeze(-1)  # Shape: [B, 256]\n",
        "        x = self.fc(x)  # Shape: [B, output_dim]\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "-W0cBkkcWyyF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch Processing for Feature Extraction\n",
        "def extract_features_in_batches(ply_folder, model, batch_size=16):\n",
        "    \"\"\"\n",
        "    Extract features from pedestrian .ply files using PointNet in batches.\n",
        "    Args:\n",
        "        ply_folder (str): Path to the folder containing .ply files.\n",
        "        model (PointNetFeatureExtractor): Trained PointNet model.\n",
        "        batch_size (int): Number of pedestrians per batch.\n",
        "    Returns:\n",
        "        dict: Dictionary of pedestrian IDs (file names) and their extracted features.\n",
        "    \"\"\"\n",
        "    # Create dataset and dataloader\n",
        "    dataset = PedestrianPointCloudDataset(ply_folder)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # Extract features\n",
        "    features = {}\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            file_names, batch_points = batch\n",
        "            batch_points_tensor = torch.stack([torch.tensor(points, dtype=torch.float32) for points in batch_points])\n",
        "            features_tensor = model(batch_points_tensor)  # Shape: [batch_size, output_dim]\n",
        "\n",
        "            # Map features to pedestrian IDs (file names)\n",
        "            for file_name, feature_vector in zip(file_names, features_tensor):\n",
        "                features[file_name] = feature_vector.numpy()\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "ufdB75OsWy0N"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Collate Function for Dataloader\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function to handle variable number of points per pedestrian.\n",
        "    Args:\n",
        "        batch (list): List of tuples (file_name, points).\n",
        "    Returns:\n",
        "        tuple: File names and padded points tensors.\n",
        "    \"\"\"\n",
        "    file_names = [item[0] for item in batch]\n",
        "    points_list = [item[1] for item in batch]\n",
        "\n",
        "    # Find the maximum number of points in the batch\n",
        "    max_points = max(points.shape[0] for points in points_list)\n",
        "\n",
        "    # Pad all points to the same size\n",
        "    padded_points = [np.pad(points, ((0, max_points - points.shape[0]), (0, 0)), mode='constant') for points in points_list]\n",
        "    return file_names, padded_points"
      ],
      "metadata": {
        "id": "cYWI114cWy2S"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_features_by_scenario(features, output_directory):\n",
        "    \"\"\"\n",
        "    Save extracted features to individual JSON files for each scenario.\n",
        "    Args:\n",
        "        features (dict): Dictionary of pedestrian IDs (file names) and their extracted features.\n",
        "        output_directory (str): Path to the directory to save the scenario JSON files.\n",
        "    \"\"\"\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "    # Organize features by scenario\n",
        "    scenario_dict = {}\n",
        "    for file_name, feature_vector in features.items():\n",
        "        # Extract scenario identifier from file_name\n",
        "        base_name = os.path.basename(file_name)\n",
        "        scenario_id = base_name.split('_')[0]  # First three digits\n",
        "        frame_number = base_name.split('_')[1]  # Frame number\n",
        "\n",
        "        # Initialize scenario entry if not already present\n",
        "        if scenario_id not in scenario_dict:\n",
        "            scenario_dict[scenario_id] = {}\n",
        "\n",
        "        # Store the frame's features\n",
        "        scenario_dict[scenario_id][f\"frame_{frame_number}\"] = feature_vector.tolist()\n",
        "\n",
        "    # Save each scenario's data into separate JSON files\n",
        "    for scenario_id, frames_data in scenario_dict.items():\n",
        "        output_file = os.path.join(output_directory, f'scenario_{scenario_id}.json')\n",
        "        with open(output_file, 'w') as json_file:\n",
        "            json.dump(frames_data, json_file, indent=4)\n",
        "        print(f\"Scenario {scenario_id} features saved to {output_file}\")"
      ],
      "metadata": {
        "id": "mah8za-JbfOi"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated batch feature extraction with saving by scenario\n",
        "def extract_and_save_features_by_scenario(ply_folder, model, batch_size, output_directory):\n",
        "    \"\"\"\n",
        "    Extract features from pedestrian .ply files using PointNet and save by scenario.\n",
        "    Args:\n",
        "        ply_folder (str): Path to the folder containing .ply files.\n",
        "        model (PointNetFeatureExtractor): Trained PointNet model.\n",
        "        batch_size (int): Number of pedestrians per batch.\n",
        "        output_directory (str): Directory to save the scenario JSON files.\n",
        "    \"\"\"\n",
        "    # Extract features\n",
        "    features = extract_features_in_batches(ply_folder, model, batch_size)\n",
        "\n",
        "    # Save features by scenario\n",
        "    save_features_by_scenario(features, output_directory)"
      ],
      "metadata": {
        "id": "JmWeOGDBZ_UT"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "pointnet_model = PointNetFeatureExtractor(input_dim=3, output_dim=64)\n",
        "\n",
        "# Path to the folder containing .ply files\n",
        "ply_folder = \"/content/drive/MyDrive/Loki_Dataset/saved_pedestrians\"\n",
        "\n",
        "# Output JSON file path\n",
        "scenario_output_directory = \"/content/drive/MyDrive/Loki_Dataset/extracted_features_point_cloud\"\n",
        "\n",
        "# Extract features and save by scenario\n",
        "extract_and_save_features_by_scenario(ply_folder, pointnet_model, batch_size=2, output_directory=scenario_output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21eopYTpWy7-",
        "outputId": "a363127e-26fc-4c9a-d60e-30701b1ea686"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scenario 000 features saved to /content/drive/MyDrive/Loki_Dataset/extracted_features_point_cloud/scenario_000.json\n",
            "Scenario 014 features saved to /content/drive/MyDrive/Loki_Dataset/extracted_features_point_cloud/scenario_014.json\n",
            "Scenario 026 features saved to /content/drive/MyDrive/Loki_Dataset/extracted_features_point_cloud/scenario_026.json\n"
          ]
        }
      ]
    }
  ]
}