{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import os\n",
    "# import torch.nn as nn\n",
    "import open3d as o3d\n",
    "# import numpy as np\n",
    "# from torch.utilities.data import Dataset, DataLoader, random_split\n",
    "# from torch_snippets.torch_loader import Report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Read_ply(Dataset):\n",
    "    def __init__(self, ply_path):\n",
    "        super(Read_ply, self).__init__()\n",
    "        self.ply_paths = ply_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ply_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        points = np.asarray(o3d.io.read_point_cloud(self.ply_paths[index]).points)\n",
    "        return points\n",
    "\n",
    "    def collate_fn(self, points):\n",
    "        max_points = max([p.shape[0] for p in points])  # Find the maximum number of points in the batch\n",
    "        \n",
    "        # Pad or truncate each point cloud to the same size (max_points)\n",
    "        padded_points = []\n",
    "        for point_cloud in points:\n",
    "            if point_cloud.shape[0] < max_points:\n",
    "                # Pad the point cloud with zeros to match the maximum size\n",
    "                padded = np.pad(point_cloud, ((0, max_points - point_cloud.shape[0]), (0, 0)), mode='constant')\n",
    "                padded_points.append(padded)\n",
    "\n",
    "        # Stack the point clouds to form a batch\n",
    "        points = np.stack(padded_points, axis=0)  # (B, N, 3)\n",
    "\n",
    "        # Create the target as the next frame\n",
    "        target = np.roll(points, shift=-1, axis=0)  # Shift the points by one frame along the batch dimension\n",
    "        target[-1] = points[-1]  # Set the last target to the last frame (no next frame for it)\n",
    "\n",
    "        # Convert to tensors\n",
    "        points = torch.tensor(points).float()\n",
    "        target = torch.tensor(target).float()\n",
    "\n",
    "        return points, target              \n",
    "\n",
    "def RandomSplit(datasets, train_set_percentage):\n",
    "    lengths = [int(len(datasets)*train_set_percentage), len(datasets)-int(len(datasets)*train_set_percentage)]\n",
    "    return random_split(datasets, lengths)\n",
    "\n",
    "\n",
    "def GetDataLoader(ply_paths, batch_size, train_set_percentage=0.9, shuffle=True, drop_last=True):\n",
    "    # Defining the dataset\n",
    "    ds = Read_ply(ply_paths)\n",
    "    # Randomly splitting the dataset\n",
    "    train_set, val_set = RandomSplit(ds, train_set_percentage)\n",
    "\n",
    "    # Defining the dataloader\n",
    "    val_dl = DataLoader(val_set, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, collate_fn=ds.collate_fn)\n",
    "    train_dl = DataLoader(train_set, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, collate_fn=ds.collate_fn)\n",
    "    \n",
    "    return train_dl, val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2796887, 3]) torch.Size([1, 2796887, 3])\n"
     ]
    }
   ],
   "source": [
    "ply_paths = [f'data/ped_4ff8af4d-6840-47c2-bc9b-eb383009ad65/frame_{i}.ply' for i in range(0,20,2)]\n",
    "\n",
    "train_dl, val_dl = GetDataLoader(ply_paths, 2)\n",
    "\n",
    "for batch in train_dl:\n",
    "    print(batch[0].shape, batch[1].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Vector Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, embed_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(3, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, embed_dim)  # Transform to higher-dimensional space\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input shape x: (batch_size, num_points, 3)\n",
    "        Output shape: (batch_size, num_points, embed_dim)\n",
    "        \"\"\"\n",
    "        out = self.mlp(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttentionMLP(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_size):\n",
    "        super(AttentionMLP, self).__init__()\n",
    "        self.att_mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, 1), # Scalar score  for each point\n",
    "            nn.Softmax(dim=1) # Normalize scores across points\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input shape x: (batch_size, num_points, embed_dim)\n",
    "        Output shape: (batch_size, num_points, 1)\n",
    "        \"\"\"\n",
    "        out = self.att_mlp(x)\n",
    "        return out\n",
    "    \n",
    "class AttentionVector(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_size, num_heads=2, dropout=0.1):\n",
    "        super(AttentionVector, self).__init__()\n",
    "        self.feature_mlp = MLP(hidden_size, embed_dim)\n",
    "        self.att_mlp = AttentionMLP(embed_dim, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input shape x: (batch_size, num_points, 3)\n",
    "        Output shape: (batch_size, num_points, embed_dim)\n",
    "        \"\"\"\n",
    "        features = self.feature_mlp(x) # -> (batch_size, num_points, embed_dim)\n",
    "        attention_scores = self.att_mlp(features) # -> (batch_size, num_points, 1)\n",
    "        wei = attention_scores * features # -> (batch_size, num_points, embed_dim)\n",
    "\n",
    "        return wei \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233 parameters\n"
     ]
    }
   ],
   "source": [
    "model = AttentionVector(embed_dim=16, hidden_size=32)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()), \"parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not Necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, optimizer, criterion, tr_dl):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for input, target in tr_dl:    \n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad(set_to_none= True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        average_loss = total_loss / len(tr_dl)\n",
    "\n",
    "    return average_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_batch(model, criterion, val_dl):\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    for input, target in val_dl:\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss.item()\n",
    "        average_loss = total_loss / len(val_dl)\n",
    "\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1.000  trn_loss: 45.607  (87.42s - 349.68s remaining)\n",
      "EPOCH: 2.000  trn_loss: 45.074  (178.77s - 268.16s remaining)\n",
      "EPOCH: 3.000  trn_loss: 45.562  (366.82s - 244.54s remaining)\n",
      "EPOCH: 3.250  trn_loss: 46.449  (399.30s - 215.01s remaining)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m N\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(train_dl)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ix, _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_dl):\n\u001B[1;32m----> 7\u001B[0m   avg_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m   log\u001B[38;5;241m.\u001B[39mrecord(epoch\u001B[38;5;241m+\u001B[39m(ix\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m/\u001B[39mN, trn_loss\u001B[38;5;241m=\u001B[39m avg_loss, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     10\u001B[0m val_loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m\n",
      "Cell \u001B[1;32mIn[17], line 4\u001B[0m, in \u001B[0;36mtrain_batch\u001B[1;34m(model, optimizer, criterion, tr_dl)\u001B[0m\n\u001B[0;32m      2\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m      3\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m----> 4\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtr_dl\u001B[49m\u001B[43m:\u001B[49m\u001B[43m    \u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\Users\\youss\\miniconda3\\envs\\pcp-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    707\u001B[0m ):\n",
      "File \u001B[1;32mc:\\Users\\youss\\miniconda3\\envs\\pcp-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mc:\\Users\\youss\\miniconda3\\envs\\pcp-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__getitems__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__:\n\u001B[1;32m---> 50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "File \u001B[1;32mc:\\Users\\youss\\miniconda3\\envs\\pcp-env\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001B[0m, in \u001B[0;36mSubset.__getitems__\u001B[1;34m(self, indices)\u001B[0m\n\u001B[0;32m    418\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m    419\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 420\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32mc:\\Users\\youss\\miniconda3\\envs\\pcp-env\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    418\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m    419\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 420\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "Cell \u001B[1;32mIn[3], line 10\u001B[0m, in \u001B[0;36mRead_ply.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, index):\n\u001B[1;32m---> 10\u001B[0m     points \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(\u001B[43mo3d\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_point_cloud\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mply_paths\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mpoints)\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m points\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "log = Report(n_epochs)\n",
    "for epoch in range(n_epochs):\n",
    "  N=len(train_dl)\n",
    "\n",
    "  for ix, _ in enumerate(train_dl):\n",
    "    avg_loss = train_batch(model, optimizer, criterion, train_dl)\n",
    "    log.record(epoch+(ix+1)/N, trn_loss= avg_loss, end='\\r')\n",
    "\n",
    "  val_loss=0\n",
    "  N=len(val_dl)\n",
    "  for ix, _ in enumerate(val_dl):\n",
    "\n",
    "    loss= val_batch(model, criterion, val_dl)\n",
    "    val_loss+= loss\n",
    "    log.record(epoch+(ix+1)/N, val_loss=loss, end='\\r')\n",
    "\n",
    "  log.report_avgs(epoch+1)\n",
    "log.plot_epochs(['trn_loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2796887, 3])\n"
     ]
    }
   ],
   "source": [
    "pcd_points = o3d.io.read_point_cloud(r\"data\\ped_4ff8af4d-6840-47c2-bc9b-eb383009ad65\\frame_0.ply\").points\n",
    "\n",
    "pcd_points = torch.tensor(np.asarray(pcd_points), dtype=torch.float32)\n",
    "\n",
    "out = model(pcd_points)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2796887, 3) float64\n"
     ]
    }
   ],
   "source": [
    "np_points = (out).detach().numpy()\n",
    "np_points=np_points.astype(np.float64)\n",
    "print(np_points.shape, np_points.dtype)\n",
    "\n",
    "points = o3d.utility.Vector3dVector(np_points)\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = points\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
